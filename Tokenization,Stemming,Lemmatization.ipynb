{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv1vGmkD6TKKmQmQtg5tkE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaison-1920/nlp/blob/main/Tokenization%2CStemming%2CLemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using NLTK library"
      ],
      "metadata": {
        "id": "v_ajy1H8L8PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "N__RPoB9MGIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN1WjRgvMEjW",
        "outputId": "4d0255ed-7534-4337-c8a5-830645d84793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\" Hello! Welcome, My name is Jaison Philip.\n",
        "I love to study A.I. Artificail intelligence is world's new trend.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xHuWFuaIMOu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9FbwCE9M2_w",
        "outputId": "8ac6b8ed-93dc-4d24-97d8-0be7e92bca01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello! Welcome, My name is Jaison Philip.\n",
            "I love to study A.I. Artificail intelligence is world's new trend.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using sentence tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "document = sent_tokenize(corpus)\n",
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBtXJtLlM4rZ",
        "outputId": "c6da337f-09d8-468d-b2f1-ebb63e32f7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Hello!',\n",
              " 'Welcome, My name is Jaison Philip.',\n",
              " 'I love to study A.I.',\n",
              " \"Artificail intelligence is world's new trend.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using word tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRYJFVRgNDHu",
        "outputId": "3b7f8bdc-6172-42dd-a4d2-9277a906cad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Jaison',\n",
              " 'Philip',\n",
              " '.',\n",
              " 'I',\n",
              " 'love',\n",
              " 'to',\n",
              " 'study',\n",
              " 'A.I',\n",
              " '.',\n",
              " 'Artificail',\n",
              " 'intelligence',\n",
              " 'is',\n",
              " 'world',\n",
              " \"'s\",\n",
              " 'new',\n",
              " 'trend',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in document:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIAyDBuJNS5E",
        "outputId": "c94ea17d-533a-40d5-8cff-e5784256df4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello!\n",
            "Welcome, My name is Jaison Philip.\n",
            "I love to study A.I.\n",
            "Artificail intelligence is world's new trend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in document:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElBNa7zAOOod",
        "outputId": "393ab76f-e3f0-41af-a151-6d5b1781545d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!']\n",
            "['Welcome', ',', 'My', 'name', 'is', 'Jaison', 'Philip', '.']\n",
            "['I', 'love', 'to', 'study', 'A.I', '.']\n",
            "['Artificail', 'intelligence', 'is', 'world', \"'s\", 'new', 'trend', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word_punct_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)\n",
        "# it will tokenize each and every word with  punctuation marks\n",
        "# earlier A.I was a single word in word_tokenize but those are\n",
        "# tokenized in wordpunct_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBT549gZOZCY",
        "outputId": "d64a3dd0-182b-4741-b676-51a70e5bf596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Jaison',\n",
              " 'Philip',\n",
              " '.',\n",
              " 'I',\n",
              " 'love',\n",
              " 'to',\n",
              " 'study',\n",
              " 'A',\n",
              " '.',\n",
              " 'I',\n",
              " '.',\n",
              " 'Artificail',\n",
              " 'intelligence',\n",
              " 'is',\n",
              " 'world',\n",
              " \"'\",\n",
              " 's',\n",
              " 'new',\n",
              " 'trend',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TreeBankWordTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "#first create an obj\n",
        "tree = TreebankWordTokenizer()\n",
        "tree.tokenize(corpus)\n",
        "# in this the last sentence only the . is considered as punctuation\n",
        "# rest of the sentences end with ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw5OwQt0OnvR",
        "outputId": "ce4a8aa3-8db3-47eb-dccf-24cbad5a542c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Jaison',\n",
              " 'Philip.',\n",
              " 'I',\n",
              " 'love',\n",
              " 'to',\n",
              " 'study',\n",
              " 'A.I.',\n",
              " 'Artificail',\n",
              " 'intelligence',\n",
              " 'is',\n",
              " 'world',\n",
              " \"'s\",\n",
              " 'new',\n",
              " 'trend',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "#### Stemming is cutting the prefixes or suffixes of a word and converting it into root stem\n",
        "#### eg: eating --> eat\n",
        "#### ===================\n",
        "#### porterStemmer, snowballStemmer, regexpStemmer etc are some examples"
      ],
      "metadata": {
        "id": "UiWWZp3YPv1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "eHeP8IAePen2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['eaten','eating','writing','eats','writes','programs','programming','history','finally','finalize']\n",
        "stemmer = PorterStemmer()\n",
        "for word in words:\n",
        "  print(word+\"---->\"+stemmer.stem(word))\n",
        "# some words like eaten,history is not getting proper words"
      ],
      "metadata": {
        "id": "ASjgtuMCoBGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7022b1f-d1cc-47ce-81e8-db783c3a3948"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eaten---->eaten\n",
            "eating---->eat\n",
            "writing---->write\n",
            "eats---->eat\n",
            "writes---->write\n",
            "programs---->program\n",
            "programming---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalize---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$|en$')\n",
        "for word in words:\n",
        "  print(word+\"---->\"+reg_stemmer.stem(word))\n",
        "#here the stemmer follow the rules blindly based on the regex\n",
        "#we are providing. so it is not so effective"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jro9nixzo3bc",
        "outputId": "69e11d65-74c1-4011-c7ba-6bfddadebcc9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eaten---->eat\n",
            "eating---->eat\n",
            "writing---->writ\n",
            "eats---->eat\n",
            "writes---->write\n",
            "programs---->program\n",
            "programming---->programm\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalize---->finaliz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snow = SnowballStemmer('english')\n",
        "for word in words:\n",
        "  print(word+\"---->\"+snow.stem(word))\n",
        "#yet there are some problems because in stemming\n",
        "#we are not getting the root word. we are getting\n",
        "#the root stem. but snowball is far better than porter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIS_3wc4p5t9",
        "outputId": "b6885a98-14e2-41f9-de94-581d4ed13260"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eaten---->eaten\n",
            "eating---->eat\n",
            "writing---->write\n",
            "eats---->eat\n",
            "writes---->write\n",
            "programs---->program\n",
            "programming---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalize---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using porter stemmer\n",
        "stemmer.stem('fairly'),stemmer.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sln6ku5zrJ6j",
        "outputId": "d69602a7-da1f-4320-ec42-4b1afdd97fbe"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using snowball stemmer\n",
        "snow.stem('fairly'),snow.stem('sportingly')\n",
        "#more better"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKKaPOCXsaAy",
        "outputId": "887fb8e8-426c-4335-d134-2a1fed8b25e0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "#### It is the process of cutting down a word to its root word\n",
        "#### eg: eating ---> eat\n",
        "#### eg: ate ----> eat\n",
        "#### WordnetLemmatizer in nltk is used for lemmatization\n",
        "#### it have 2 parameters---> the word and 'pos'"
      ],
      "metadata": {
        "id": "WIuu4ZOCsnkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('eating',pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "q48SXkx5sieH",
        "outputId": "3464c641-5b2c-470d-e03d-da0cb2a77222"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('ate',pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HYt-Wbg1uT-L",
        "outputId": "44f8f6ef-f5c8-45ac-dd1d-0de4866cf4a2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('pleasent',pos='n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x2rNglaoudKw",
        "outputId": "05eb2f5e-c866-4758-ddb7-037ddb80a3af"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pleasent'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('went',pos='v')\n",
        "# v-> verb\n",
        "# a-> adjective\n",
        "# n-> noun\n",
        "# r-> adverb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_YyJh8b-uhxs",
        "outputId": "47639a21-980b-4601-8113-b21dfd8ed4b1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTYtU5UWvM-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}